{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210e4ed3-2ce1-47f9-91de-a96d7e407c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_posts_df = spark.read.table('data_platform.project.raw_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb36b1d-d0c1-4bc5-b9fc-22958e129e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(raw_posts_df.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2746e545-54fa-4e32-8e3f-eb429925c42b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Declarative transformations"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType \n",
    "from pyspark.sql import DataFrame \n",
    "\n",
    "def split_tags_to_array(df: DataFrame) -> DataFrame:\n",
    "    return(\n",
    "        df.withColumn('TagArray', F.filter(F.split(F.col('Tags'), r'\\|'), lambda x: x!= '')).drop(F.col('Tags'))\n",
    "    ) \n",
    "\n",
    "def rename_column(df: DataFrame) -> DataFrame:\n",
    "    return(\n",
    "        df.withColumnRenamed('Id', 'PostId')\n",
    "    ) \n",
    "\n",
    "def map_posts_to_id(df: DataFrame) -> DataFrame:\n",
    "    map_data = [\n",
    "        (1, \"Question\"),\n",
    "        (2, \"Answer\"),\n",
    "        (3, \"Orphaned tag wiki\"),\n",
    "        (4, \"Tag wiki excerpt\"),\n",
    "        (5, \"Tag wiki\"),\n",
    "        (6, \"Moderator nomination\"),\n",
    "        (7, \"Wiki placeholder\"),\n",
    "        (8, \"Privilege wiki\"),\n",
    "        (9, \"Article\"),\n",
    "        (10, \"HelpArticle\"),\n",
    "        (12, \"Collection\"),\n",
    "        (13, \"ModeratorQuestionnaireResponse\"),\n",
    "        (14, \"Announcement\"),\n",
    "        (15, \"CollectiveDiscussion\"),\n",
    "        (17, \"CollectiveCollection\")\n",
    "    ]\n",
    "    map_schema = StructType([\n",
    "        StructField('PostTypeId', IntegerType(), False),\n",
    "        StructField('PostType', StringType(), False)\n",
    "    ])\n",
    "    map_df = spark.createDataFrame(map_data, schema = map_schema)\n",
    "\n",
    "    return(\n",
    "        df.join(\n",
    "            F.broadcast(map_df),\n",
    "            on = 'PostTypeId',\n",
    "            how = 'left'\n",
    "        )\n",
    "    )\n",
    "\n",
    "stg_posts_df = (\n",
    "    raw_posts_df.transform(split_tags_to_array)\n",
    "    .transform(rename_column)\n",
    "    .transform(map_posts_to_id)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fb8d5e-ae34-4472-8fd6-3cfe57f88255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def incremental_upsert(dest_table: str, df: DataFrame, unique_key: str, updated_at: str, full_refresh = False):\n",
    "    if not spark.catalog.tableExists(dest_table) or full_refresh:\n",
    "        (\n",
    "            df.write\n",
    "                .format('delta')\n",
    "                .mode('overwrite')\n",
    "                .option('overwriteSchema', 'true')\n",
    "                .saveAsTable(dest_table)\n",
    "        )\n",
    "    else:\n",
    "        latest_max = (\n",
    "            spark.read.table(dest_table)\n",
    "                .agg(F.max(updated_at).alias('max_ts'))\n",
    "                .collect()[0]['max_ts']\n",
    "        )\n",
    "        incremental_df = df.filter(F.col(updated_at) > latest_max)\n",
    "\n",
    "        if not incremental_df.rdd.isEmpty():\n",
    "            delta_table = DeltaTable.forName(spark, dest_table)\n",
    "\n",
    "            (\n",
    "                delta_table.alias('d').merge(\n",
    "                    source = incremental_df.alias('i'),\n",
    "                    condition = f'd.{unique_key} == i.{unique_key}'\n",
    "                )\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "            )\n",
    "\n",
    "dest_table = 'data_platform.project.stg_posts'\n",
    "incremental_upsert(dest_table, stg_posts_df, 'PostId', 'CreationDate')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c3c2cb-cce4-40e1-b1fa-385b2b90834f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(dest_table).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aab5fd3-0a74-40a1-91c5-a33e00ca67e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stg_posts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
